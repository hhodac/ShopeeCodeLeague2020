{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/shopee-sentiment-analysis/train.csv\n/kaggle/input/shopee-sentiment-analysis/sampleSubmission.csv\n/kaggle/input/shopee-sentiment-analysis/test.csv\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# 1. Load data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\npath = '/kaggle/input/shopee-sentiment-analysis/'\ntrain_df = pd.read_csv('/kaggle/input/shopee-sentiment-analysis/train.csv')\ntest_df = pd.read_csv('/kaggle/input/shopee-sentiment-analysis/test.csv')\nsampleSubmission_df = pd.read_csv('/kaggle/input/shopee-sentiment-analysis/sampleSubmission.csv')","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Preprocess data"},{"metadata":{},"cell_type":"markdown","source":"## 2.1. Filtering emoji"},{"metadata":{"trusted":true},"cell_type":"code","source":"import emoji  # https://pypi.org/project/emoji/\n\ndef emoji_cleaning(text):\n    # Change emoji to text\n    text = emoji.demojize(text).replace(\":\", \" \")\n    \n    # Delete repeated emoji\n    tokenizer = text.split()\n    repeated_list = []\n    \n    for word in tokenizer:\n        if word not in repeated_list:\n            repeated_list.append(word)\n    \n    text = ' '.join(text for text in repeated_list)\n    text = text.replace(\"_\", \" \").replace(\"-\", \" \")\n    return text","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"have_emoji_train_idx = []\nhave_emoji_test_idx = []\n\nfor idx, review in enumerate(train_df['review']):\n    if any(char in emoji.UNICODE_EMOJI for char in review):\n        have_emoji_train_idx.append(idx)\n        \nfor idx, review in enumerate(test_df['review']):\n    if any(char in emoji.UNICODE_EMOJI for char in review):\n        have_emoji_test_idx.append(idx)\n        \n# emoji_cleaning\ntrain_df.loc[have_emoji_train_idx, 'review'] = train_df.loc[have_emoji_train_idx, 'review'].apply(emoji_cleaning)\ntest_df.loc[have_emoji_test_idx, 'review'] = test_df.loc[have_emoji_test_idx, 'review'].apply(emoji_cleaning)","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2.2. Cleaning text\n* emotion\n* repeated word characters (in bahasa)\n* punctuation\n* shortened words\n* noisy text"},{"metadata":{"trusted":true},"cell_type":"code","source":"import re # regular expression\n\ndef review_cleaning(text):\n    \n    # delete lowercase and newline\n    text = text.lower()\n    text = re.sub(r'\\n', '', text)\n    \n    # change emoticon to text\n    text = re.sub(r':\\(', 'dislike', text)\n    text = re.sub(r': \\(\\(', 'dislike', text)\n    text = re.sub(r':, \\(', 'dislike', text)\n    text = re.sub(r':\\)', 'smile', text)\n    text = re.sub(r';\\)', 'smile', text)\n    text = re.sub(r':\\)\\)\\)', 'smile', text)\n    text = re.sub(r':\\)\\)\\)\\)\\)\\)', 'smile', text)\n    text = re.sub(r'=\\)\\)\\)\\)', 'smile', text)\n    \n    # delete punctuation\n    text = re.sub('[^a-z0-9 ]', ' ', text)\n    \n    tokenizer = text.split()\n    \n    return ' '.join([text for text in tokenizer])\n\ntrain_df['review'] = train_df['review'].apply(review_cleaning)\ntest_df['review'] = test_df['review'].apply(review_cleaning)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"repeated_rows_train = []\nrepeated_rows_test = []\n\nfor idx, review in enumerate(train_df['review']):\n    if re.match(r'\\w*(\\w)\\1+', review):\n        repeated_rows_train.append(idx)\n        \nfor idx, review in enumerate(test_df['review']):\n    if re.match(r'\\w*(\\w)\\1+', review):\n        repeated_rows_test.append(idx)\n        \ndef delete_repeated_char(text):\n    text = re.sub(r'(\\w)\\1{2,}', r'\\1', text)\n    return text\n\ntrain_df.loc[repeated_rows_train, 'review'] = train_df.loc[repeated_rows_train, 'review'].apply(delete_repeated_char)\ntest_df.loc[repeated_rows_test, 'review'] = test_df.loc[repeated_rows_test, 'review'].apply(delete_repeated_char)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop the row that has noisy text or mislabeled rating\nnoisy_row = [31, 50, 2235, 5244, 10409, 11748, 12384, 14395, 15215, 17629, 20819, 23691, 32089, 39532, 40530, 43954, 48186, 50500, 55834, 60088,\n             60442, 61095, 62982, 63803, 67464, 70791, 74861, 73636, 74119, 76275, 79789, 85745, 91058, 91663, 91800, 93204, 99295, 100903, 101177, 103155,\n             109166, 109566, 109651, 109724, 110115, 110441, 111461, 113175, 115782, 116903, 118099, 118328, 118414, 119071, 125338, 125340, 129496, 129640, \n             132027, 138212, 131626, 134715, 133248, 136217, 141377, 143707, 145045, 146485, 37301]\n\ntrain_df.drop(noisy_row, inplace=True)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def recover_shortened_words(text):\n    \n    # put \\b (boundary) for avoid the characters in the word to be replaced\n    # I only make a few examples here, you can add if you're interested :)\n    \n    text = re.sub(r'\\bapaa\\b', 'apa', text)\n    \n    text = re.sub(r'\\bbsk\\b', 'besok', text)\n    text = re.sub(r'\\bbrngnya\\b', 'barangnya', text)\n    text = re.sub(r'\\bbrp\\b', 'berapa', text)\n    text = re.sub(r'\\bbgt\\b', 'banget', text)\n    text = re.sub(r'\\bbngt\\b', 'banget', text)\n    text = re.sub(r'\\bgini\\b', 'begini', text)\n    text = re.sub(r'\\bbrg\\b', 'barang', text)\n    \n    text = re.sub(r'\\bdtg\\b', 'datang', text)\n    text = re.sub(r'\\bd\\b', 'di', text)\n    text = re.sub(r'\\bsdh\\b', 'sudah', text)\n    text = re.sub(r'\\bdri\\b', 'dari', text)\n    text = re.sub(r'\\bdsni\\b', 'disini', text)\n    \n    text = re.sub(r'\\bgk\\b', 'gak', text)\n    \n    text = re.sub(r'\\bhrs\\b', 'harus', text)\n    \n    text = re.sub(r'\\bjd\\b', 'jadi', text)\n    text = re.sub(r'\\bjg\\b', 'juga', text)\n    text = re.sub(r'\\bjgn\\b', 'jangan', text)\n    \n    text = re.sub(r'\\blg\\b', 'lagi', text)\n    text = re.sub(r'\\blgi\\b', 'lagi', text)\n    text = re.sub(r'\\blbh\\b', 'lebih', text)\n    text = re.sub(r'\\blbih\\b', 'lebih', text)\n    \n    text = re.sub(r'\\bmksh\\b', 'makasih', text)\n    text = re.sub(r'\\bmna\\b', 'mana', text)\n    \n    text = re.sub(r'\\borg\\b', 'orang', text)\n    \n    text = re.sub(r'\\bpjg\\b', 'panjang', text)\n    \n    text = re.sub(r'\\bka\\b', 'kakak', text)\n    text = re.sub(r'\\bkk\\b', 'kakak', text)\n    text = re.sub(r'\\bklo\\b', 'kalau', text)\n    text = re.sub(r'\\bkmrn\\b', 'kemarin', text)\n    text = re.sub(r'\\bkmrin\\b', 'kemarin', text)\n    text = re.sub(r'\\bknp\\b', 'kenapa', text)\n    text = re.sub(r'\\bkcil\\b', 'kecil', text)\n    \n    text = re.sub(r'\\bgmn\\b', 'gimana', text)\n    text = re.sub(r'\\bgmna\\b', 'gimana', text)\n    \n    text = re.sub(r'\\btp\\b', 'tapi', text)\n    text = re.sub(r'\\btq\\b', 'thanks', text)\n    text = re.sub(r'\\btks\\b', 'thanks', text)\n    text = re.sub(r'\\btlg\\b', 'tolong', text)\n    text = re.sub(r'\\bgk\\b', 'tidak', text)\n    text = re.sub(r'\\bgak\\b', 'tidak', text)\n    text = re.sub(r'\\bgpp\\b', 'tidak apa apa', text)\n    text = re.sub(r'\\bgapapa\\b', 'tidak apa apa', text)\n    text = re.sub(r'\\bga\\b', 'tidak', text)\n    text = re.sub(r'\\btgl\\b', 'tanggal', text)\n    text = re.sub(r'\\btggl\\b', 'tanggal', text)\n    text = re.sub(r'\\bgamau\\b', 'tidak mau', text)\n    \n    text = re.sub(r'\\bsy\\b', 'saya', text)\n    text = re.sub(r'\\bsis\\b', 'sister', text)\n    text = re.sub(r'\\bsdgkan\\b', 'sedangkan', text)\n    text = re.sub(r'\\bmdh2n\\b', 'semoga', text)\n    text = re.sub(r'\\bsmoga\\b', 'semoga', text)\n    text = re.sub(r'\\bsmpai\\b', 'sampai', text)\n    text = re.sub(r'\\bnympe\\b', 'sampai', text)\n    text = re.sub(r'\\bdah\\b', 'sudah', text)\n    \n    text = re.sub(r'\\bberkali2\\b', 'repeated', text)\n    \n    text = re.sub(r'\\byg\\b', 'yang', text)\n    \n    return text\n\ntrain_df['review'] = train_df['review'].apply(recover_shortened_words)","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Build Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.text import *\nbs=8\ndata_clas = (TextList.from_csv(path, csv_name='train.csv', cols='review')\n        .split_by_rand_pct(0.1)\n        .label_from_df(cols='rating')\n        .add_test(TextList.from_csv(path, csv_name='test.csv', cols='review'))\n        .databunch(bs=bs))","execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# 4. Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn = text_classifier_learner(data_clas, Transformer, drop_mult=0.4, model_dir='/kaggle/output', pretrained=False)","execution_count":37,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"'str' object has no attribute 'to'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-6ef9a71edb26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# learn = text_classifier_learner(data_clas, Transformer, drop_mult=0.4, model_dir='/kaggle/output', pretrained=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNNLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_clas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lstm_wt103'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/fastai/text/learner.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, model, split_func, clip, alpha, beta, metrics, **learn_kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m                                                      isinstance(data.train_ds.y, LMLabelList)))\n\u001b[1;32m     51\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mifnone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_class\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mlearn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNNTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGradientClipping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, model, opt_func, loss_func, metrics, true_wd, bn_wd, wd, train_bn, path, model_dir, callback_fns, callbacks, layer_groups, add_time, silent)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36m__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;34m\"Setup path,metrics, callbacks and ensure model directory exists.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mifnone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'to'"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()\nlearn.recorder.plot()","execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      0.00% [0/1 00:00<00:00]\n    </div>\n    \n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>\n\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='95' class='' max='16516' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      0.58% [95/16516 00:11<32:20 5.2906]\n    </div>\n    "},"metadata":{}},{"output_type":"stream","text":"LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{},"cell_type":"markdown","source":"# 5. Train model"},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(1, 5e-2, moms=(0.8,0.7))","execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1.260983</td>\n      <td>1.269235</td>\n      <td>0.411893</td>\n      <td>10:24</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('first')","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.load('first')","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"RNNLearner(data=TextClasDataBunch;\n\nTrain: LabelList (132130 items)\nx: TextList\nxxbos xxmaj xxunk replace broken glass , broken chargernya,xxbos xxmaj nyesel bngt dsni shopping antecedent photo message pictures gk according xxunk existing collagen super fit nyampe xxunk my house open ehhh collagen contents even in the face pdahal jg description super existing collagen xxunk writing my check lg in photo captions already ma xxmaj the change ma pictures that the face .,xxbos xxmaj sent a light blue suit goods ga want a refund,xxbos xxmaj pendants came with dents and scratches on its surface . xxmaj the coating looks like it will change colour quickly .,xxbos xxmaj dg yg depending being sent in photos\ny: CategoryList\n1,1,1,1,1\nPath: /kaggle/input/shopee-sentiment-analysis;\n\nValid: LabelList (14681 items)\nx: TextList\nxxbos xxmaj two people order but your mic . xxmaj just one night without a charger and xxmaj mike ung ... xxmaj please arrange for wire xxmaj naman check first on bgo ideliver nyo ... ang packaging xxmaj xxunk well ... so ung color i ordered ...,xxbos xxmaj good quality , good value for money to buy a new one . xxmaj classy xxwrep 23 mm mm . mm mm mm,xxbos xxmaj the product quality is not good . xxmaj the product quality is not good . xxmaj the product quality is not good .,xxbos h xxrep 5 a xxrep 4 r ggg xxrep 4 a xxunk xxrep 4 r xxrep 4 a xxrep 4 h xxunk xxrep 4 a xxunk xxrep 4 a xxrep 4 s bbb xxrep 4 a xxrep 4 g xxrep 4 u xxrep 5 s,xxbos xxmaj cosmetic pouch nyaa good very fast delivery .. xxmaj according hargaaa . xxmaj thanks kakkk .\ny: CategoryList\n1,4,1,5,3\nPath: /kaggle/input/shopee-sentiment-analysis;\n\nTest: LabelList (60427 items)\nx: TextList\nxxbos xxmaj great danger , cool , motif and cantik2 jg models . xxmaj delivery cepet . xxmaj tp packing less okay krn only wear clear plastic nerawang xxunk contents jd,xxbos xxmaj one of the shades do n't fit well,xxbos xxmaj very comfortable,xxbos xxmaj fast delivery . xxmaj product expiry is on xxmaj dec 2022 . xxmaj product wrap properly . xxmaj no damage on the item .,xxbos it 's s xxrep 5 o cute ! i like playing with the xxunk better than xxunk on my phone now . item was also xxunk earlier than i expected . thank you seller ! may you have more buyers to come . ðŸ˜Š ðŸ˜Š ðŸ˜Š\ny: EmptyLabelList\n,,,,\nPath: /kaggle/input/shopee-sentiment-analysis, model=SequentialRNN(\n  (0): MultiBatchEncoder(\n    (module): Transformer(\n      (encoder): Embedding(16192, 768)\n      (pos_enc): Embedding(512, 768)\n      (drop_emb): Dropout(p=0.04000000000000001, inplace=False)\n      (layers): ModuleList(\n        (0): DecoderLayer(\n          (mhra): MultiHeadAttention(\n            (attention): Linear(in_features=768, out_features=2304, bias=True)\n            (out): Linear(in_features=768, out_features=768, bias=True)\n            (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n            (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (ff): SequentialEx(\n            (layers): ModuleList(\n              (0): Linear(in_features=768, out_features=3072, bias=True)\n              (1): GeLU()\n              (2): Linear(in_features=3072, out_features=768, bias=True)\n              (3): Dropout(p=0.04000000000000001, inplace=False)\n              (4): MergeLayer()\n              (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (1): DecoderLayer(\n          (mhra): MultiHeadAttention(\n            (attention): Linear(in_features=768, out_features=2304, bias=True)\n            (out): Linear(in_features=768, out_features=768, bias=True)\n            (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n            (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (ff): SequentialEx(\n            (layers): ModuleList(\n              (0): Linear(in_features=768, out_features=3072, bias=True)\n              (1): GeLU()\n              (2): Linear(in_features=3072, out_features=768, bias=True)\n              (3): Dropout(p=0.04000000000000001, inplace=False)\n              (4): MergeLayer()\n              (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (2): DecoderLayer(\n          (mhra): MultiHeadAttention(\n            (attention): Linear(in_features=768, out_features=2304, bias=True)\n            (out): Linear(in_features=768, out_features=768, bias=True)\n            (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n            (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (ff): SequentialEx(\n            (layers): ModuleList(\n              (0): Linear(in_features=768, out_features=3072, bias=True)\n              (1): GeLU()\n              (2): Linear(in_features=3072, out_features=768, bias=True)\n              (3): Dropout(p=0.04000000000000001, inplace=False)\n              (4): MergeLayer()\n              (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (3): DecoderLayer(\n          (mhra): MultiHeadAttention(\n            (attention): Linear(in_features=768, out_features=2304, bias=True)\n            (out): Linear(in_features=768, out_features=768, bias=True)\n            (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n            (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (ff): SequentialEx(\n            (layers): ModuleList(\n              (0): Linear(in_features=768, out_features=3072, bias=True)\n              (1): GeLU()\n              (2): Linear(in_features=3072, out_features=768, bias=True)\n              (3): Dropout(p=0.04000000000000001, inplace=False)\n              (4): MergeLayer()\n              (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (4): DecoderLayer(\n          (mhra): MultiHeadAttention(\n            (attention): Linear(in_features=768, out_features=2304, bias=True)\n            (out): Linear(in_features=768, out_features=768, bias=True)\n            (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n            (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (ff): SequentialEx(\n            (layers): ModuleList(\n              (0): Linear(in_features=768, out_features=3072, bias=True)\n              (1): GeLU()\n              (2): Linear(in_features=3072, out_features=768, bias=True)\n              (3): Dropout(p=0.04000000000000001, inplace=False)\n              (4): MergeLayer()\n              (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (5): DecoderLayer(\n          (mhra): MultiHeadAttention(\n            (attention): Linear(in_features=768, out_features=2304, bias=True)\n            (out): Linear(in_features=768, out_features=768, bias=True)\n            (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n            (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (ff): SequentialEx(\n            (layers): ModuleList(\n              (0): Linear(in_features=768, out_features=3072, bias=True)\n              (1): GeLU()\n              (2): Linear(in_features=3072, out_features=768, bias=True)\n              (3): Dropout(p=0.04000000000000001, inplace=False)\n              (4): MergeLayer()\n              (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (6): DecoderLayer(\n          (mhra): MultiHeadAttention(\n            (attention): Linear(in_features=768, out_features=2304, bias=True)\n            (out): Linear(in_features=768, out_features=768, bias=True)\n            (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n            (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (ff): SequentialEx(\n            (layers): ModuleList(\n              (0): Linear(in_features=768, out_features=3072, bias=True)\n              (1): GeLU()\n              (2): Linear(in_features=3072, out_features=768, bias=True)\n              (3): Dropout(p=0.04000000000000001, inplace=False)\n              (4): MergeLayer()\n              (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (7): DecoderLayer(\n          (mhra): MultiHeadAttention(\n            (attention): Linear(in_features=768, out_features=2304, bias=True)\n            (out): Linear(in_features=768, out_features=768, bias=True)\n            (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n            (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (ff): SequentialEx(\n            (layers): ModuleList(\n              (0): Linear(in_features=768, out_features=3072, bias=True)\n              (1): GeLU()\n              (2): Linear(in_features=3072, out_features=768, bias=True)\n              (3): Dropout(p=0.04000000000000001, inplace=False)\n              (4): MergeLayer()\n              (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (8): DecoderLayer(\n          (mhra): MultiHeadAttention(\n            (attention): Linear(in_features=768, out_features=2304, bias=True)\n            (out): Linear(in_features=768, out_features=768, bias=True)\n            (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n            (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (ff): SequentialEx(\n            (layers): ModuleList(\n              (0): Linear(in_features=768, out_features=3072, bias=True)\n              (1): GeLU()\n              (2): Linear(in_features=3072, out_features=768, bias=True)\n              (3): Dropout(p=0.04000000000000001, inplace=False)\n              (4): MergeLayer()\n              (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (9): DecoderLayer(\n          (mhra): MultiHeadAttention(\n            (attention): Linear(in_features=768, out_features=2304, bias=True)\n            (out): Linear(in_features=768, out_features=768, bias=True)\n            (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n            (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (ff): SequentialEx(\n            (layers): ModuleList(\n              (0): Linear(in_features=768, out_features=3072, bias=True)\n              (1): GeLU()\n              (2): Linear(in_features=3072, out_features=768, bias=True)\n              (3): Dropout(p=0.04000000000000001, inplace=False)\n              (4): MergeLayer()\n              (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (10): DecoderLayer(\n          (mhra): MultiHeadAttention(\n            (attention): Linear(in_features=768, out_features=2304, bias=True)\n            (out): Linear(in_features=768, out_features=768, bias=True)\n            (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n            (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (ff): SequentialEx(\n            (layers): ModuleList(\n              (0): Linear(in_features=768, out_features=3072, bias=True)\n              (1): GeLU()\n              (2): Linear(in_features=3072, out_features=768, bias=True)\n              (3): Dropout(p=0.04000000000000001, inplace=False)\n              (4): MergeLayer()\n              (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (11): DecoderLayer(\n          (mhra): MultiHeadAttention(\n            (attention): Linear(in_features=768, out_features=2304, bias=True)\n            (out): Linear(in_features=768, out_features=768, bias=True)\n            (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n            (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (ff): SequentialEx(\n            (layers): ModuleList(\n              (0): Linear(in_features=768, out_features=3072, bias=True)\n              (1): GeLU()\n              (2): Linear(in_features=3072, out_features=768, bias=True)\n              (3): Dropout(p=0.04000000000000001, inplace=False)\n              (4): MergeLayer()\n              (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n      )\n    )\n  )\n  (1): PoolingLinearClassifier(\n    (layers): Sequential(\n      (0): BatchNorm1d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (1): Linear(in_features=2304, out_features=50, bias=True)\n      (2): ReLU(inplace=True)\n      (3): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (4): Dropout(p=0.1, inplace=False)\n      (5): Linear(in_features=50, out_features=5, bias=True)\n    )\n  )\n), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7fd7e89c1b00>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/kaggle/input/shopee-sentiment-analysis'), model_dir='/kaggle/output', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\nlearn: ...\nalpha: 2.0\nbeta: 1.0], layer_groups=[Sequential(\n  (0): Embedding(16192, 768)\n), Sequential(\n  (0): DecoderLayer(\n    (mhra): MultiHeadAttention(\n      (attention): Linear(in_features=768, out_features=2304, bias=True)\n      (out): Linear(in_features=768, out_features=768, bias=True)\n      (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n      (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (ff): SequentialEx(\n      (layers): ModuleList(\n        (0): Linear(in_features=768, out_features=3072, bias=True)\n        (1): GeLU()\n        (2): Linear(in_features=3072, out_features=768, bias=True)\n        (3): Dropout(p=0.04000000000000001, inplace=False)\n        (4): MergeLayer()\n        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (1): DecoderLayer(\n    (mhra): MultiHeadAttention(\n      (attention): Linear(in_features=768, out_features=2304, bias=True)\n      (out): Linear(in_features=768, out_features=768, bias=True)\n      (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n      (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (ff): SequentialEx(\n      (layers): ModuleList(\n        (0): Linear(in_features=768, out_features=3072, bias=True)\n        (1): GeLU()\n        (2): Linear(in_features=3072, out_features=768, bias=True)\n        (3): Dropout(p=0.04000000000000001, inplace=False)\n        (4): MergeLayer()\n        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (2): DecoderLayer(\n    (mhra): MultiHeadAttention(\n      (attention): Linear(in_features=768, out_features=2304, bias=True)\n      (out): Linear(in_features=768, out_features=768, bias=True)\n      (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n      (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (ff): SequentialEx(\n      (layers): ModuleList(\n        (0): Linear(in_features=768, out_features=3072, bias=True)\n        (1): GeLU()\n        (2): Linear(in_features=3072, out_features=768, bias=True)\n        (3): Dropout(p=0.04000000000000001, inplace=False)\n        (4): MergeLayer()\n        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (3): DecoderLayer(\n    (mhra): MultiHeadAttention(\n      (attention): Linear(in_features=768, out_features=2304, bias=True)\n      (out): Linear(in_features=768, out_features=768, bias=True)\n      (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n      (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (ff): SequentialEx(\n      (layers): ModuleList(\n        (0): Linear(in_features=768, out_features=3072, bias=True)\n        (1): GeLU()\n        (2): Linear(in_features=3072, out_features=768, bias=True)\n        (3): Dropout(p=0.04000000000000001, inplace=False)\n        (4): MergeLayer()\n        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n), Sequential(\n  (0): DecoderLayer(\n    (mhra): MultiHeadAttention(\n      (attention): Linear(in_features=768, out_features=2304, bias=True)\n      (out): Linear(in_features=768, out_features=768, bias=True)\n      (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n      (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (ff): SequentialEx(\n      (layers): ModuleList(\n        (0): Linear(in_features=768, out_features=3072, bias=True)\n        (1): GeLU()\n        (2): Linear(in_features=3072, out_features=768, bias=True)\n        (3): Dropout(p=0.04000000000000001, inplace=False)\n        (4): MergeLayer()\n        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (1): DecoderLayer(\n    (mhra): MultiHeadAttention(\n      (attention): Linear(in_features=768, out_features=2304, bias=True)\n      (out): Linear(in_features=768, out_features=768, bias=True)\n      (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n      (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (ff): SequentialEx(\n      (layers): ModuleList(\n        (0): Linear(in_features=768, out_features=3072, bias=True)\n        (1): GeLU()\n        (2): Linear(in_features=3072, out_features=768, bias=True)\n        (3): Dropout(p=0.04000000000000001, inplace=False)\n        (4): MergeLayer()\n        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (2): DecoderLayer(\n    (mhra): MultiHeadAttention(\n      (attention): Linear(in_features=768, out_features=2304, bias=True)\n      (out): Linear(in_features=768, out_features=768, bias=True)\n      (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n      (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (ff): SequentialEx(\n      (layers): ModuleList(\n        (0): Linear(in_features=768, out_features=3072, bias=True)\n        (1): GeLU()\n        (2): Linear(in_features=3072, out_features=768, bias=True)\n        (3): Dropout(p=0.04000000000000001, inplace=False)\n        (4): MergeLayer()\n        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (3): DecoderLayer(\n    (mhra): MultiHeadAttention(\n      (attention): Linear(in_features=768, out_features=2304, bias=True)\n      (out): Linear(in_features=768, out_features=768, bias=True)\n      (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n      (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (ff): SequentialEx(\n      (layers): ModuleList(\n        (0): Linear(in_features=768, out_features=3072, bias=True)\n        (1): GeLU()\n        (2): Linear(in_features=3072, out_features=768, bias=True)\n        (3): Dropout(p=0.04000000000000001, inplace=False)\n        (4): MergeLayer()\n        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n), Sequential(\n  (0): DecoderLayer(\n    (mhra): MultiHeadAttention(\n      (attention): Linear(in_features=768, out_features=2304, bias=True)\n      (out): Linear(in_features=768, out_features=768, bias=True)\n      (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n      (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (ff): SequentialEx(\n      (layers): ModuleList(\n        (0): Linear(in_features=768, out_features=3072, bias=True)\n        (1): GeLU()\n        (2): Linear(in_features=3072, out_features=768, bias=True)\n        (3): Dropout(p=0.04000000000000001, inplace=False)\n        (4): MergeLayer()\n        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (1): DecoderLayer(\n    (mhra): MultiHeadAttention(\n      (attention): Linear(in_features=768, out_features=2304, bias=True)\n      (out): Linear(in_features=768, out_features=768, bias=True)\n      (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n      (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (ff): SequentialEx(\n      (layers): ModuleList(\n        (0): Linear(in_features=768, out_features=3072, bias=True)\n        (1): GeLU()\n        (2): Linear(in_features=3072, out_features=768, bias=True)\n        (3): Dropout(p=0.04000000000000001, inplace=False)\n        (4): MergeLayer()\n        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (2): DecoderLayer(\n    (mhra): MultiHeadAttention(\n      (attention): Linear(in_features=768, out_features=2304, bias=True)\n      (out): Linear(in_features=768, out_features=768, bias=True)\n      (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n      (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (ff): SequentialEx(\n      (layers): ModuleList(\n        (0): Linear(in_features=768, out_features=3072, bias=True)\n        (1): GeLU()\n        (2): Linear(in_features=3072, out_features=768, bias=True)\n        (3): Dropout(p=0.04000000000000001, inplace=False)\n        (4): MergeLayer()\n        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (3): DecoderLayer(\n    (mhra): MultiHeadAttention(\n      (attention): Linear(in_features=768, out_features=2304, bias=True)\n      (out): Linear(in_features=768, out_features=768, bias=True)\n      (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n      (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (ff): SequentialEx(\n      (layers): ModuleList(\n        (0): Linear(in_features=768, out_features=3072, bias=True)\n        (1): GeLU()\n        (2): Linear(in_features=3072, out_features=768, bias=True)\n        (3): Dropout(p=0.04000000000000001, inplace=False)\n        (4): MergeLayer()\n        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n), Sequential(\n  (0): PoolingLinearClassifier(\n    (layers): Sequential(\n      (0): BatchNorm1d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (1): Linear(in_features=2304, out_features=50, bias=True)\n      (2): ReLU(inplace=True)\n      (3): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (4): Dropout(p=0.1, inplace=False)\n      (5): Linear(in_features=50, out_features=5, bias=True)\n    )\n  )\n)], add_time=True, silent=False)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.lr_find()\nlearn.recorder.plot()","execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      0.00% [0/1 00:00<00:00]\n    </div>\n    \n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>\n\n    <div>\n        <style>\n            /* Turns off some styling */\n            progress {\n                /* gets rid of default border in Firefox and Opera. */\n                border: none;\n                /* Needs to be in here for Safari polyfill so background images work as expected. */\n                background-size: auto;\n            }\n            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n                background: #F44336;\n            }\n        </style>\n      <progress value='86' class='' max='16516' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      0.52% [86/16516 00:10<32:27 3.2697]\n    </div>\n    "},"metadata":{}},{"output_type":"stream","text":"LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.freeze_to(-2)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.fit_one_cycle(1, slice(5e-4/(2.6**4), 5e-4), moms=(0.8,0.7))","execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>epoch</th>\n      <th>train_loss</th>\n      <th>valid_loss</th>\n      <th>accuracy</th>\n      <th>time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1.274258</td>\n      <td>1.158922</td>\n      <td>0.439820</td>\n      <td>12:45</td>\n    </tr>\n  </tbody>\n</table>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.save('second')","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"learn.load('second')","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"RNNLearner(data=TextClasDataBunch;\n\nTrain: LabelList (132130 items)\nx: TextList\nxxbos xxmaj xxunk replace broken glass , broken chargernya,xxbos xxmaj nyesel bngt dsni shopping antecedent photo message pictures gk according xxunk existing collagen super fit nyampe xxunk my house open ehhh collagen contents even in the face pdahal jg description super existing collagen xxunk writing my check lg in photo captions already ma xxmaj the change ma pictures that the face .,xxbos xxmaj sent a light blue suit goods ga want a refund,xxbos xxmaj pendants came with dents and scratches on its surface . xxmaj the coating looks like it will change colour quickly .,xxbos xxmaj dg yg depending being sent in photos\ny: CategoryList\n1,1,1,1,1\nPath: /kaggle/input/shopee-sentiment-analysis;\n\nValid: LabelList (14681 items)\nx: TextList\nxxbos xxmaj two people order but your mic . xxmaj just one night without a charger and xxmaj mike ung ... xxmaj please arrange for wire xxmaj naman check first on bgo ideliver nyo ... ang packaging xxmaj xxunk well ... so ung color i ordered ...,xxbos xxmaj good quality , good value for money to buy a new one . xxmaj classy xxwrep 23 mm mm . mm mm mm,xxbos xxmaj the product quality is not good . xxmaj the product quality is not good . xxmaj the product quality is not good .,xxbos h xxrep 5 a xxrep 4 r ggg xxrep 4 a xxunk xxrep 4 r xxrep 4 a xxrep 4 h xxunk xxrep 4 a xxunk xxrep 4 a xxrep 4 s bbb xxrep 4 a xxrep 4 g xxrep 4 u xxrep 5 s,xxbos xxmaj cosmetic pouch nyaa good very fast delivery .. xxmaj according hargaaa . xxmaj thanks kakkk .\ny: CategoryList\n1,4,1,5,3\nPath: /kaggle/input/shopee-sentiment-analysis;\n\nTest: LabelList (60427 items)\nx: TextList\nxxbos xxmaj great danger , cool , motif and cantik2 jg models . xxmaj delivery cepet . xxmaj tp packing less okay krn only wear clear plastic nerawang xxunk contents jd,xxbos xxmaj one of the shades do n't fit well,xxbos xxmaj very comfortable,xxbos xxmaj fast delivery . xxmaj product expiry is on xxmaj dec 2022 . xxmaj product wrap properly . xxmaj no damage on the item .,xxbos it 's s xxrep 5 o cute ! i like playing with the xxunk better than xxunk on my phone now . item was also xxunk earlier than i expected . thank you seller ! may you have more buyers to come . ðŸ˜Š ðŸ˜Š ðŸ˜Š\ny: EmptyLabelList\n,,,,\nPath: /kaggle/input/shopee-sentiment-analysis, model=SequentialRNN(\n  (0): MultiBatchEncoder(\n    (module): Transformer(\n      (encoder): Embedding(16192, 768)\n      (pos_enc): Embedding(512, 768)\n      (drop_emb): Dropout(p=0.04000000000000001, inplace=False)\n      (layers): ModuleList(\n        (0): DecoderLayer(\n          (mhra): MultiHeadAttention(\n            (attention): Linear(in_features=768, out_features=2304, bias=True)\n            (out): Linear(in_features=768, out_features=768, bias=True)\n            (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n            (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (ff): SequentialEx(\n            (layers): ModuleList(\n              (0): Linear(in_features=768, out_features=3072, bias=True)\n              (1): GeLU()\n              (2): Linear(in_features=3072, out_features=768, bias=True)\n              (3): Dropout(p=0.04000000000000001, inplace=False)\n              (4): MergeLayer()\n              (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (1): DecoderLayer(\n          (mhra): MultiHeadAttention(\n            (attention): Linear(in_features=768, out_features=2304, bias=True)\n            (out): Linear(in_features=768, out_features=768, bias=True)\n            (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n            (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (ff): SequentialEx(\n            (layers): ModuleList(\n              (0): Linear(in_features=768, out_features=3072, bias=True)\n              (1): GeLU()\n              (2): Linear(in_features=3072, out_features=768, bias=True)\n              (3): Dropout(p=0.04000000000000001, inplace=False)\n              (4): MergeLayer()\n              (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (2): DecoderLayer(\n          (mhra): MultiHeadAttention(\n            (attention): Linear(in_features=768, out_features=2304, bias=True)\n            (out): Linear(in_features=768, out_features=768, bias=True)\n            (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n            (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (ff): SequentialEx(\n            (layers): ModuleList(\n              (0): Linear(in_features=768, out_features=3072, bias=True)\n              (1): GeLU()\n              (2): Linear(in_features=3072, out_features=768, bias=True)\n              (3): Dropout(p=0.04000000000000001, inplace=False)\n              (4): MergeLayer()\n              (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (3): DecoderLayer(\n          (mhra): MultiHeadAttention(\n            (attention): Linear(in_features=768, out_features=2304, bias=True)\n            (out): Linear(in_features=768, out_features=768, bias=True)\n            (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n            (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (ff): SequentialEx(\n            (layers): ModuleList(\n              (0): Linear(in_features=768, out_features=3072, bias=True)\n              (1): GeLU()\n              (2): Linear(in_features=3072, out_features=768, bias=True)\n              (3): Dropout(p=0.04000000000000001, inplace=False)\n              (4): MergeLayer()\n              (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (4): DecoderLayer(\n          (mhra): MultiHeadAttention(\n            (attention): Linear(in_features=768, out_features=2304, bias=True)\n            (out): Linear(in_features=768, out_features=768, bias=True)\n            (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n            (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (ff): SequentialEx(\n            (layers): ModuleList(\n              (0): Linear(in_features=768, out_features=3072, bias=True)\n              (1): GeLU()\n              (2): Linear(in_features=3072, out_features=768, bias=True)\n              (3): Dropout(p=0.04000000000000001, inplace=False)\n              (4): MergeLayer()\n              (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (5): DecoderLayer(\n          (mhra): MultiHeadAttention(\n            (attention): Linear(in_features=768, out_features=2304, bias=True)\n            (out): Linear(in_features=768, out_features=768, bias=True)\n            (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n            (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (ff): SequentialEx(\n            (layers): ModuleList(\n              (0): Linear(in_features=768, out_features=3072, bias=True)\n              (1): GeLU()\n              (2): Linear(in_features=3072, out_features=768, bias=True)\n              (3): Dropout(p=0.04000000000000001, inplace=False)\n              (4): MergeLayer()\n              (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (6): DecoderLayer(\n          (mhra): MultiHeadAttention(\n            (attention): Linear(in_features=768, out_features=2304, bias=True)\n            (out): Linear(in_features=768, out_features=768, bias=True)\n            (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n            (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (ff): SequentialEx(\n            (layers): ModuleList(\n              (0): Linear(in_features=768, out_features=3072, bias=True)\n              (1): GeLU()\n              (2): Linear(in_features=3072, out_features=768, bias=True)\n              (3): Dropout(p=0.04000000000000001, inplace=False)\n              (4): MergeLayer()\n              (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (7): DecoderLayer(\n          (mhra): MultiHeadAttention(\n            (attention): Linear(in_features=768, out_features=2304, bias=True)\n            (out): Linear(in_features=768, out_features=768, bias=True)\n            (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n            (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (ff): SequentialEx(\n            (layers): ModuleList(\n              (0): Linear(in_features=768, out_features=3072, bias=True)\n              (1): GeLU()\n              (2): Linear(in_features=3072, out_features=768, bias=True)\n              (3): Dropout(p=0.04000000000000001, inplace=False)\n              (4): MergeLayer()\n              (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (8): DecoderLayer(\n          (mhra): MultiHeadAttention(\n            (attention): Linear(in_features=768, out_features=2304, bias=True)\n            (out): Linear(in_features=768, out_features=768, bias=True)\n            (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n            (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (ff): SequentialEx(\n            (layers): ModuleList(\n              (0): Linear(in_features=768, out_features=3072, bias=True)\n              (1): GeLU()\n              (2): Linear(in_features=3072, out_features=768, bias=True)\n              (3): Dropout(p=0.04000000000000001, inplace=False)\n              (4): MergeLayer()\n              (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (9): DecoderLayer(\n          (mhra): MultiHeadAttention(\n            (attention): Linear(in_features=768, out_features=2304, bias=True)\n            (out): Linear(in_features=768, out_features=768, bias=True)\n            (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n            (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (ff): SequentialEx(\n            (layers): ModuleList(\n              (0): Linear(in_features=768, out_features=3072, bias=True)\n              (1): GeLU()\n              (2): Linear(in_features=3072, out_features=768, bias=True)\n              (3): Dropout(p=0.04000000000000001, inplace=False)\n              (4): MergeLayer()\n              (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (10): DecoderLayer(\n          (mhra): MultiHeadAttention(\n            (attention): Linear(in_features=768, out_features=2304, bias=True)\n            (out): Linear(in_features=768, out_features=768, bias=True)\n            (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n            (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (ff): SequentialEx(\n            (layers): ModuleList(\n              (0): Linear(in_features=768, out_features=3072, bias=True)\n              (1): GeLU()\n              (2): Linear(in_features=3072, out_features=768, bias=True)\n              (3): Dropout(p=0.04000000000000001, inplace=False)\n              (4): MergeLayer()\n              (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (11): DecoderLayer(\n          (mhra): MultiHeadAttention(\n            (attention): Linear(in_features=768, out_features=2304, bias=True)\n            (out): Linear(in_features=768, out_features=768, bias=True)\n            (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n            (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n            (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          )\n          (ff): SequentialEx(\n            (layers): ModuleList(\n              (0): Linear(in_features=768, out_features=3072, bias=True)\n              (1): GeLU()\n              (2): Linear(in_features=3072, out_features=768, bias=True)\n              (3): Dropout(p=0.04000000000000001, inplace=False)\n              (4): MergeLayer()\n              (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n      )\n    )\n  )\n  (1): PoolingLinearClassifier(\n    (layers): Sequential(\n      (0): BatchNorm1d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (1): Linear(in_features=2304, out_features=50, bias=True)\n      (2): ReLU(inplace=True)\n      (3): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (4): Dropout(p=0.1, inplace=False)\n      (5): Linear(in_features=50, out_features=5, bias=True)\n    )\n  )\n), opt_func=functools.partial(<class 'torch.optim.adam.Adam'>, betas=(0.9, 0.99)), loss_func=FlattenedLoss of CrossEntropyLoss(), metrics=[<function accuracy at 0x7fd7e89c1b00>], true_wd=True, bn_wd=True, wd=0.01, train_bn=True, path=PosixPath('/kaggle/input/shopee-sentiment-analysis'), model_dir='/kaggle/output', callback_fns=[functools.partial(<class 'fastai.basic_train.Recorder'>, add_time=True, silent=False)], callbacks=[RNNTrainer\nlearn: ...\nalpha: 2.0\nbeta: 1.0], layer_groups=[Sequential(\n  (0): Embedding(16192, 768)\n), Sequential(\n  (0): DecoderLayer(\n    (mhra): MultiHeadAttention(\n      (attention): Linear(in_features=768, out_features=2304, bias=True)\n      (out): Linear(in_features=768, out_features=768, bias=True)\n      (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n      (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (ff): SequentialEx(\n      (layers): ModuleList(\n        (0): Linear(in_features=768, out_features=3072, bias=True)\n        (1): GeLU()\n        (2): Linear(in_features=3072, out_features=768, bias=True)\n        (3): Dropout(p=0.04000000000000001, inplace=False)\n        (4): MergeLayer()\n        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (1): DecoderLayer(\n    (mhra): MultiHeadAttention(\n      (attention): Linear(in_features=768, out_features=2304, bias=True)\n      (out): Linear(in_features=768, out_features=768, bias=True)\n      (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n      (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (ff): SequentialEx(\n      (layers): ModuleList(\n        (0): Linear(in_features=768, out_features=3072, bias=True)\n        (1): GeLU()\n        (2): Linear(in_features=3072, out_features=768, bias=True)\n        (3): Dropout(p=0.04000000000000001, inplace=False)\n        (4): MergeLayer()\n        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (2): DecoderLayer(\n    (mhra): MultiHeadAttention(\n      (attention): Linear(in_features=768, out_features=2304, bias=True)\n      (out): Linear(in_features=768, out_features=768, bias=True)\n      (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n      (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (ff): SequentialEx(\n      (layers): ModuleList(\n        (0): Linear(in_features=768, out_features=3072, bias=True)\n        (1): GeLU()\n        (2): Linear(in_features=3072, out_features=768, bias=True)\n        (3): Dropout(p=0.04000000000000001, inplace=False)\n        (4): MergeLayer()\n        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (3): DecoderLayer(\n    (mhra): MultiHeadAttention(\n      (attention): Linear(in_features=768, out_features=2304, bias=True)\n      (out): Linear(in_features=768, out_features=768, bias=True)\n      (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n      (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (ff): SequentialEx(\n      (layers): ModuleList(\n        (0): Linear(in_features=768, out_features=3072, bias=True)\n        (1): GeLU()\n        (2): Linear(in_features=3072, out_features=768, bias=True)\n        (3): Dropout(p=0.04000000000000001, inplace=False)\n        (4): MergeLayer()\n        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n), Sequential(\n  (0): DecoderLayer(\n    (mhra): MultiHeadAttention(\n      (attention): Linear(in_features=768, out_features=2304, bias=True)\n      (out): Linear(in_features=768, out_features=768, bias=True)\n      (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n      (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (ff): SequentialEx(\n      (layers): ModuleList(\n        (0): Linear(in_features=768, out_features=3072, bias=True)\n        (1): GeLU()\n        (2): Linear(in_features=3072, out_features=768, bias=True)\n        (3): Dropout(p=0.04000000000000001, inplace=False)\n        (4): MergeLayer()\n        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (1): DecoderLayer(\n    (mhra): MultiHeadAttention(\n      (attention): Linear(in_features=768, out_features=2304, bias=True)\n      (out): Linear(in_features=768, out_features=768, bias=True)\n      (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n      (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (ff): SequentialEx(\n      (layers): ModuleList(\n        (0): Linear(in_features=768, out_features=3072, bias=True)\n        (1): GeLU()\n        (2): Linear(in_features=3072, out_features=768, bias=True)\n        (3): Dropout(p=0.04000000000000001, inplace=False)\n        (4): MergeLayer()\n        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (2): DecoderLayer(\n    (mhra): MultiHeadAttention(\n      (attention): Linear(in_features=768, out_features=2304, bias=True)\n      (out): Linear(in_features=768, out_features=768, bias=True)\n      (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n      (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (ff): SequentialEx(\n      (layers): ModuleList(\n        (0): Linear(in_features=768, out_features=3072, bias=True)\n        (1): GeLU()\n        (2): Linear(in_features=3072, out_features=768, bias=True)\n        (3): Dropout(p=0.04000000000000001, inplace=False)\n        (4): MergeLayer()\n        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (3): DecoderLayer(\n    (mhra): MultiHeadAttention(\n      (attention): Linear(in_features=768, out_features=2304, bias=True)\n      (out): Linear(in_features=768, out_features=768, bias=True)\n      (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n      (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (ff): SequentialEx(\n      (layers): ModuleList(\n        (0): Linear(in_features=768, out_features=3072, bias=True)\n        (1): GeLU()\n        (2): Linear(in_features=3072, out_features=768, bias=True)\n        (3): Dropout(p=0.04000000000000001, inplace=False)\n        (4): MergeLayer()\n        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n), Sequential(\n  (0): DecoderLayer(\n    (mhra): MultiHeadAttention(\n      (attention): Linear(in_features=768, out_features=2304, bias=True)\n      (out): Linear(in_features=768, out_features=768, bias=True)\n      (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n      (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (ff): SequentialEx(\n      (layers): ModuleList(\n        (0): Linear(in_features=768, out_features=3072, bias=True)\n        (1): GeLU()\n        (2): Linear(in_features=3072, out_features=768, bias=True)\n        (3): Dropout(p=0.04000000000000001, inplace=False)\n        (4): MergeLayer()\n        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (1): DecoderLayer(\n    (mhra): MultiHeadAttention(\n      (attention): Linear(in_features=768, out_features=2304, bias=True)\n      (out): Linear(in_features=768, out_features=768, bias=True)\n      (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n      (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (ff): SequentialEx(\n      (layers): ModuleList(\n        (0): Linear(in_features=768, out_features=3072, bias=True)\n        (1): GeLU()\n        (2): Linear(in_features=3072, out_features=768, bias=True)\n        (3): Dropout(p=0.04000000000000001, inplace=False)\n        (4): MergeLayer()\n        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (2): DecoderLayer(\n    (mhra): MultiHeadAttention(\n      (attention): Linear(in_features=768, out_features=2304, bias=True)\n      (out): Linear(in_features=768, out_features=768, bias=True)\n      (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n      (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (ff): SequentialEx(\n      (layers): ModuleList(\n        (0): Linear(in_features=768, out_features=3072, bias=True)\n        (1): GeLU()\n        (2): Linear(in_features=3072, out_features=768, bias=True)\n        (3): Dropout(p=0.04000000000000001, inplace=False)\n        (4): MergeLayer()\n        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (3): DecoderLayer(\n    (mhra): MultiHeadAttention(\n      (attention): Linear(in_features=768, out_features=2304, bias=True)\n      (out): Linear(in_features=768, out_features=768, bias=True)\n      (drop_att): Dropout(p=0.04000000000000001, inplace=False)\n      (drop_res): Dropout(p=0.04000000000000001, inplace=False)\n      (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    )\n    (ff): SequentialEx(\n      (layers): ModuleList(\n        (0): Linear(in_features=768, out_features=3072, bias=True)\n        (1): GeLU()\n        (2): Linear(in_features=3072, out_features=768, bias=True)\n        (3): Dropout(p=0.04000000000000001, inplace=False)\n        (4): MergeLayer()\n        (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n), Sequential(\n  (0): PoolingLinearClassifier(\n    (layers): Sequential(\n      (0): BatchNorm1d(2304, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (1): Linear(in_features=2304, out_features=50, bias=True)\n      (2): ReLU(inplace=True)\n      (3): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (4): Dropout(p=0.1, inplace=False)\n      (5): Linear(in_features=50, out_features=5, bias=True)\n    )\n  )\n)], add_time=True, silent=False)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# 6. Make prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"preds, _ = learn.get_preds(ds_type=DatasetType.Test, ordered=True)\nratings = np.argmax(preds, 1)\nprint(min(ratings))","execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"stream","text":"tensor(0)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# 7. Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_preds = ratings + 1\nsubmission_df = pd.DataFrame({'review_id': test_df['review_id'], 'rating': submission_preds})\nsubmission_df.to_csv('submission.csv', header=True, index=False)","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.head()","execution_count":28,"outputs":[{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"   review_id  rating\n0          1       3\n1          2       3\n2          3       5\n3          4       3\n4          5       5","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review_id</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df.hist()","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fd739d14b10>,\n        <matplotlib.axes._subplots.AxesSubplot object at 0x7fd739d16790>]], dtype=object)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 2 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}